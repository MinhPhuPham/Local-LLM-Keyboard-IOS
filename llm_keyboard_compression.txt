# LLM Keyboard: Advanced Compression & Integration

## 1. Model Encryption & Compression (Like MOZC)

### How MOZC Achieves 2-3MB Size

MOZC uses several advanced compression techniques:

#### A. LOUDS (Level-Order Unary Degree Sequence) Trie
MOZC stores dictionaries in a highly compressed trie structure:

```
Traditional Trie:     ~50MB
Double-Array Trie:    ~20MB
LOUDS Trie:           ~3MB  ‚Üê MOZC uses this
```

**Why LOUDS is so efficient:**
- Represents entire tree with just 2 bit arrays
- 7x smaller than pointer-based tries
- Still allows fast O(k) lookup (k = key length)

#### B. Your LLM Model Compression Strategy

```python
# compression_pipeline.py

class AggressiveCompression:
    """
    Compress LLM model to <50MB for iOS keyboard
    """
    
    def step1_quantization(self, model):
        """
        4-bit quantization (most aggressive)
        """
        from transformers import BitsAndBytesConfig
        
        # 4-bit quantization
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,  # Double quantization
        )
        
        # Result: 100MB model ‚Üí 25MB
        return quantized_model
    
    def step2_vocabulary_compression(self, tokenizer, model):
        """
        Use LOUDS trie for vocabulary (like MOZC)
        """
        from louds import LOUDSTrie
        
        # Build LOUDS trie for tokenizer vocabulary
        vocab_trie = LOUDSTrie()
        for token, id in tokenizer.vocab.items():
            vocab_trie.insert(token, id)
        
        # Compress vocabulary
        # Original: 32000 tokens √ó 30 bytes = ~1MB
        # LOUDS: ~200KB (5x compression)
        
        compressed_vocab = vocab_trie.serialize()
        
        return compressed_vocab
    
    def step3_weight_pruning(self, model):
        """
        Remove unnecessary weights
        """
        import torch.nn.utils.prune as prune
        
        # Prune 30% of least important weights
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=0.3)
        
        # Result: Additional 20-30% size reduction
        return model
    
    def step4_huffman_encoding(self, quantized_weights):
        """
        Entropy encoding for quantized weights
        """
        from dahuffman import HuffmanCodec
        
        # Weights after 4-bit quantization have only 16 unique values
        # Use Huffman coding to compress further
        codec = HuffmanCodec.from_data(quantized_weights)
        compressed = codec.encode(quantized_weights)
        
        # Result: Additional 15-20% reduction
        return compressed
    
    def step5_model_distillation(self, teacher_model):
        """
        Create smaller student model (optional, but best)
        """
        from transformers import DistilBertForMaskedLM
        
        # Train a tiny student model
        student_config = {
            'num_layers': 3,      # vs 6 in teacher
            'hidden_size': 128,   # vs 256 in teacher
            'num_heads': 4,       # vs 8 in teacher
        }
        
        student = train_student(teacher_model, student_config)
        
        # Result: 50MB ‚Üí 15MB (with similar accuracy)
        return student
    
    def full_compression_pipeline(self, model, tokenizer):
        """
        Complete compression: 100MB ‚Üí 10-20MB
        """
        print("üîß Starting aggressive compression...\n")
        
        # Step 1: Distillation (optional but recommended)
        print("1. Model distillation...")
        distilled = self.step5_model_distillation(model)
        print(f"   Size: {get_size(model)}MB ‚Üí {get_size(distilled)}MB")
        
        # Step 2: 4-bit quantization
        print("2. 4-bit quantization...")
        quantized = self.step1_quantization(distilled)
        print(f"   Size: {get_size(distilled)}MB ‚Üí {get_size(quantized)}MB")
        
        # Step 3: Weight pruning
        print("3. Pruning 30% of weights...")
        pruned = self.step3_weight_pruning(quantized)
        print(f"   Size: {get_size(quantized)}MB ‚Üí {get_size(pruned)}MB")
        
        # Step 4: Vocabulary compression (LOUDS)
        print("4. Compressing vocabulary with LOUDS...")
        vocab_compressed = self.step2_vocabulary_compression(tokenizer, pruned)
        print(f"   Vocab: 1MB ‚Üí {len(vocab_compressed)/1024}KB")
        
        # Step 5: Huffman encoding
        print("5. Entropy encoding...")
        final = self.step4_huffman_encoding(pruned)
        print(f"   Size: {get_size(pruned)}MB ‚Üí {get_size(final)}MB")
        
        # Step 6: LZMA compression (for storage)
        import lzma
        compressed_bytes = lzma.compress(
            final.tobytes(), 
            preset=9
        )
        
        print(f"\n‚úÖ Final size: {len(compressed_bytes)/1024/1024:.1f}MB")
        
        return compressed_bytes, vocab_compressed


# Swift: Decompression in iOS
"""
class ModelLoader {
    func loadCompressedModel() throws -> MLModel {
        // 1. Decompress LZMA
        guard let compressedData = try? Data(contentsOf: modelURL) else {
            throw ModelError.loadFailed
        }
        
        let decompressed = try (compressedData as NSData)
            .decompressed(using: .lzma) as Data
        
        // 2. Load into memory
        let config = MLModelConfiguration()
        config.computeUnits = .cpuAndNeuralEngine
        
        // 3. Decode Huffman
        let weights = huffmanDecode(decompressed)
        
        // 4. Restore to CoreML format
        let model = try MLModel(contentsOf: modelURL, 
                                configuration: config)
        
        return model
    }
}
"""
```

### Encryption Strategy

```swift
// Swift: AES-256 encryption for model

import CryptoKit

class ModelEncryption {
    
    // Obfuscated key (don't store plaintext!)
    private static let keyMaterial: [UInt8] = [
        // XOR with app-specific values
        0x1A ^ 0x42, 0x2B ^ 0x13, // ... 32 bytes
    ]
    
    static func encryptModel(_ data: Data) throws -> Data {
        let key = SymmetricKey(data: Data(keyMaterial))
        let nonce = AES.GCM.Nonce()
        
        let sealedBox = try AES.GCM.seal(
            data, 
            using: key, 
            nonce: nonce
        )
        
        return sealedBox.combined!
    }
    
    static func decryptModel(_ encrypted: Data) throws -> Data {
        let key = SymmetricKey(data: Data(keyMaterial))
        let sealedBox = try AES.GCM.SealedBox(combined: encrypted)
        
        return try AES.GCM.open(sealedBox, using: key)
    }
}
```

---

## 2. Language-Specific Models (Japanese + English)

### Strategy: Two Separate Lightweight Models

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Japanese Mode                          ‚îÇ
‚îÇ  - JP model (70%): 35MB                 ‚îÇ
‚îÇ  - EN support (30%): 15MB               ‚îÇ
‚îÇ  - Total: 50MB                          ‚îÇ
‚îÇ  - Use case: Typing in Japanese         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  English Mode                           ‚îÇ
‚îÇ  - EN model (100%): 30MB                ‚îÇ
‚îÇ  - No JP support                        ‚îÇ
‚îÇ  - Total: 30MB                          ‚îÇ
‚îÇ  - Use case: Typing in English          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation:

```python
# Training separate models

class DualLanguageTraining:
    
    def train_japanese_model(self):
        """
        Japanese model with limited English support
        """
        corpus = {
            'japanese': load_corpus('ja', size='500M'),  # 70%
            'english': load_corpus('en', size='200M'),   # 30%
        }
        
        # Focus tokenizer on Japanese
        tokenizer = train_tokenizer(
            corpus,
            vocab_size=32000,
            character_coverage=0.9995,  # High for Japanese kanji
            model_type='unigram',       # Better for Japanese
        )
        
        model = train_transformer(
            corpus=corpus,
            tokenizer=tokenizer,
            # Optimize for Japanese
            config={
                'layers': 4,
                'hidden': 256,
                'heads': 8,
            }
        )
        
        return model, tokenizer
    
    def train_english_model(self):
        """
        English-only model (more efficient)
        """
        corpus = load_corpus('en', size='700M')
        
        # English-focused tokenizer
        tokenizer = train_tokenizer(
            corpus,
            vocab_size=16000,  # Smaller vocab for English only
            model_type='bpe',  # Better for English
        )
        
        model = train_transformer(
            corpus=corpus,
            tokenizer=tokenizer,
            config={
                'layers': 3,       # Fewer layers
                'hidden': 192,
                'heads': 6,
            }
        )
        
        return model, tokenizer


# Swift: Dynamic model loading

class LanguageAwareKeyboard: UIInputViewController {
    
    private var japaneseModel: MLModel?
    private var englishModel: MLModel?
    private var currentLanguage: Language = .japanese
    
    enum Language {
        case japanese
        case english
    }
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        // Load based on user's primary input language
        detectAndLoadModel()
    }
    
    func detectAndLoadModel() {
        // Check what user is typing
        let textProxy = self.textDocumentProxy
        let context = textProxy.documentContextBeforeInput ?? ""
        
        if containsJapanese(context) {
            loadJapaneseModel()
        } else {
            loadEnglishModel()
        }
    }
    
    func loadJapaneseModel() {
        guard japaneseModel == nil else { return }
        
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                self.japaneseModel = try MLModel(
                    contentsOf: self.japaneseModelURL
                )
                self.currentLanguage = .japanese
            } catch {
                print("Failed to load JP model: \(error)")
            }
        }
    }
    
    func loadEnglishModel() {
        guard englishModel == nil else { return }
        
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                self.englishModel = try MLModel(
                    contentsOf: self.englishModelURL
                )
                self.currentLanguage = .english
            } catch {
                print("Failed to load EN model: \(error)")
            }
        }
    }
    
    // Intelligent model switching
    func textDidChange(_ textInput: UITextInput?) {
        let context = textDocumentProxy.documentContextBeforeInput ?? ""
        
        // Auto-switch based on content
        if containsJapanese(context) && currentLanguage == .english {
            currentLanguage = .japanese
            // JP model already loaded from viewDidLoad
        } else if !containsJapanese(context) && currentLanguage == .japanese {
            currentLanguage = .english
            loadEnglishModel()  // Lazy load
        }
    }
}
```

**Benefits:**
- ‚úÖ Japanese model: Better at Japanese, still handles English
- ‚úÖ English model: Faster & more accurate for pure English
- ‚úÖ Smaller total size per mode
- ‚úÖ Better performance (smaller = faster inference)

---

## 3. Integrating User Dictionaries

### Two-Layer Dictionary System

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 1: Pre-trained Model             ‚îÇ
‚îÇ  - Frozen (never changes)               ‚îÇ
‚îÇ  - Provides base predictions            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 2: User Dictionaries             ‚îÇ
‚îÇ  - iOS System Dictionary (Read)         ‚îÇ
‚îÇ  - App Custom Dictionary (Read/Write)   ‚îÇ
‚îÇ  - Boosts relevant predictions          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation:

```swift
// User Dictionary Integration

class UserDictionaryManager {
    
    // iOS system dictionary (read-only)
    func getSystemDictionary() -> [String: Any] {
        var systemWords: [String] = []
        
        // Access iOS user dictionary
        if let checker = UITextChecker() {
            // Note: iOS doesn't provide direct API to read user dictionary
            // We need to track words as user types them
        }
        
        return ["words": systemWords]
    }
    
    // App custom dictionary (read/write)
    func getCustomDictionary() -> [String: WordInfo] {
        guard let url = FileManager.default
            .containerURL(forSecurityApplicationGroupIdentifier: "group.com.yourapp")?
            .appendingPathComponent("custom_dictionary.json") else {
            return [:]
        }
        
        guard let data = try? Data(contentsOf: url),
              let dict = try? JSONDecoder().decode(
                [String: WordInfo].self, 
                from: data
              ) else {
            return [:]
        }
        
        return dict
    }
    
    func addToCustomDictionary(word: String, reading: String? = nil) {
        var dict = getCustomDictionary()
        
        dict[word] = WordInfo(
            word: word,
            reading: reading,
            frequency: 1,
            lastUsed: Date(),
            source: .userAdded
        )
        
        saveCustomDictionary(dict)
    }
}

struct WordInfo: Codable {
    let word: String
    let reading: String?
    var frequency: Int
    var lastUsed: Date
    let source: DictionarySource
    
    enum DictionarySource: String, Codable {
        case userAdded
        case learned
        case imported
    }
}

// Integration with prediction engine

class PredictionEngine {
    
    private let baseModel: MLModel
    private let dictionaryManager = UserDictionaryManager()
    
    func getPredictions(for input: String) -> [Suggestion] {
        // 1. Get base predictions from model
        let basePredictions = baseModel.predict(input)
        
        // 2. Get user dictionary words
        let customDict = dictionaryManager.getCustomDictionary()
        
        // 3. Boost predictions that match user dictionary
        let boostedPredictions = boostWithDictionary(
            basePredictions, 
            dictionary: customDict,
            input: input
        )
        
        return boostedPredictions
    }
    
    func boostWithDictionary(
        _ predictions: [Suggestion],
        dictionary: [String: WordInfo],
        input: String
    ) -> [Suggestion] {
        
        var scored = predictions.map { pred in
            var score = pred.score
            
            // Exact match in custom dictionary
            if let wordInfo = dictionary[pred.text] {
                // Boost based on frequency
                let frequencyBoost = log(Double(wordInfo.frequency + 1)) * 0.3
                
                // Boost based on recency
                let daysSinceUsed = Date().timeIntervalSince(wordInfo.lastUsed) / 86400
                let recencyBoost = max(0, 1.0 - daysSinceUsed / 30) * 0.2
                
                score += frequencyBoost + recencyBoost
            }
            
            // Prefix match
            if pred.text.hasPrefix(input) {
                if dictionary.keys.contains(where: { $0.hasPrefix(input) }) {
                    score += 0.1
                }
            }
            
            return Suggestion(text: pred.text, score: score)
        }
        
        // Add dictionary words not in base predictions
        for (word, info) in dictionary {
            if word.hasPrefix(input) && !predictions.contains(where: { $0.text == word }) {
                let score = log(Double(info.frequency + 1)) * 0.5
                scored.append(Suggestion(text: word, score: score))
            }
        }
        
        // Sort by score
        return scored.sorted { $0.score > $1.score }
    }
    
    // Learn from user selections
    func userDidSelect(_ word: String, input: String) {
        // Add to custom dictionary if not exists
        if !dictionaryManager.getCustomDictionary().keys.contains(word) {
            dictionaryManager.addToCustomDictionary(word: word)
        } else {
            // Increment frequency
            var dict = dictionaryManager.getCustomDictionary()
            if var info = dict[word] {
                info.frequency += 1
                info.lastUsed = Date()
                dict[word] = info
                dictionaryManager.saveCustomDictionary(dict)
            }
        }
    }
}

struct Suggestion {
    let text: String
    var score: Double
}
```

### Importing User Dictionary from Settings

```swift
// Main app: Allow user to import their dictionary

class DictionaryImportViewController: UIViewController {
    
    func importFromiOSSettings() {
        // User exports their iOS dictionary
        // Format: CSV or plist
        
        let picker = UIDocumentPickerViewController(
            forOpeningContentTypes: [.commaSeparatedText]
        )
        picker.delegate = self
        present(picker, animated: true)
    }
    
    func documentPicker(_ controller: UIDocumentPickerViewController, 
                       didPickDocumentsAt urls: [URL]) {
        guard let url = urls.first else { return }
        
        // Parse CSV
        guard let content = try? String(contentsOf: url) else { return }
        let lines = content.components(separatedBy: "\n")
        
        var customDict: [String: WordInfo] = [:]
        
        for line in lines {
            let parts = line.components(separatedBy: ",")
            guard parts.count >= 2 else { continue }
            
            let word = parts[0].trimmingCharacters(in: .whitespaces)
            let reading = parts[1].trimmingCharacters(in: .whitespaces)
            
            customDict[word] = WordInfo(
                word: word,
                reading: reading,
                frequency: 1,
                lastUsed: Date(),
                source: .imported
            )
        }
        
        // Save to shared container
        let manager = UserDictionaryManager()
        manager.mergeCustomDictionary(customDict)
        
        showAlert("Imported \(customDict.count) words!")
    }
}
```

---

## 4. Limiting Suggestions to N Words

### Efficient Top-K Selection

```swift
class EfficientPredictionEngine {
    
    let maxSuggestions = 30  // Your limit
    
    func getTopPredictions(for input: String) -> [Suggestion] {
        // 1. Get raw predictions from model (might return 100+)
        let rawPredictions = baseModel.predict(input)
        
        // 2. Use heap for efficient top-K selection
        let topK = selectTopK(
            rawPredictions, 
            k: maxSuggestions,
            input: input
        )
        
        return topK
    }
    
    func selectTopK(_ predictions: [Suggestion], 
                   k: Int, 
                   input: String) -> [Suggestion] {
        
        // Min-heap for O(n log k) complexity
        var heap = MinHeap<Suggestion>(maxSize: k) { $0.score < $1.score }
        
        for pred in predictions {
            // Calculate final score
            var score = pred.score
            
            // Apply user dictionary boost
            score += getDictionaryBoost(pred.text)
            
            // Apply context boost
            score += getContextBoost(pred.text, input: input)
            
            // Apply frequency boost (from learned data)
            score += getFrequencyBoost(pred.text)
            
            // Add to heap (automatically maintains top-K)
            heap.insert(Suggestion(text: pred.text, score: score))
        }
        
        // Extract top K and sort descending
        return heap.extractAll().sorted { $0.score > $1.score }
    }
    
    // Optimization: Early termination for long suggestion lists
    func predictWithEarlyStop(input: String) -> [Suggestion] {
        var results: [Suggestion] = []
        let threshold = 0.1  // Minimum score to consider
        
        // Stream predictions from model
        for pred in baseModel.streamPredictions(input) {
            guard pred.score > threshold else {
                // Stop early if scores drop too low
                break
            }
            
            results.append(pred)
            
            if results.count >= maxSuggestions * 2 {
                // Got enough candidates, stop querying model
                break
            }
        }
        
        return selectTopK(results, k: maxSuggestions, input: input)
    }
}

// Min-heap implementation for top-K
class MinHeap<T> {
    private var heap: [T] = []
    private let compare: (T, T) -> Bool
    private let maxSize: Int
    
    init(maxSize: Int, compare: @escaping (T, T) -> Bool) {
        self.maxSize = maxSize
        self.compare = compare
    }
    
    func insert(_ element: T) {
        if heap.count < maxSize {
            heap.append(element)
            bubbleUp(heap.count - 1)
        } else if !compare(element, heap[0]) {
            // Replace minimum if new element is larger
            heap[0] = element
            bubbleDown(0)
        }
    }
    
    func extractAll() -> [T] {
        return heap
    }
    
    private func bubbleUp(_ index: Int) {
        var child = index
        while child > 0 {
            let parent = (child - 1) / 2
            if compare(heap[child], heap[parent]) {
                heap.swapAt(child, parent)
                child = parent
            } else {
                break
            }
        }
    }
    
    private func bubbleDown(_ index: Int) {
        var parent = index
        while true {
            let left = 2 * parent + 1
            let right = 2 * parent + 2
            var smallest = parent
            
            if left < heap.count && compare(heap[left], heap[smallest]) {
                smallest = left
            }
            if right < heap.count && compare(heap[right], heap[smallest]) {
                smallest = right
            }
            
            if smallest != parent {
                heap.swapAt(parent, smallest)
                parent = smallest
            } else {
                break
            }
        }
    }
}
```

### Performance Optimization for Keyboard

```swift
class OptimizedKeyboardEngine {
    
    // Cache frequent predictions
    private var cache = LRUCache<String, [Suggestion]>(capacity: 100)
    
    // Debounce user input
    private var debouncer = Debouncer(delay: 0.1)  // 100ms
    
    func getSuggestions(for input: String, 
                       completion: @escaping ([Suggestion]) -> Void) {
        
        // 1. Check cache first (instant)
        if let cached = cache.get(input) {
            completion(cached)
            return
        }
        
        // 2. Debounce rapid typing
        debouncer.debounce {
            // 3. Get predictions asynchronously
            DispatchQueue.global(qos: .userInteractive).async {
                let predictions = self.predictWithTimeout(input, timeout: 0.05)
                
                // 4. Cache result
                self.cache.set(input, predictions)
                
                // 5. Return on main thread
                DispatchQueue.main.async {
                    completion(predictions)
                }
            }
        }
    }
    
    func predictWithTimeout(_ input: String, timeout: TimeInterval) -> [Suggestion] {
        let start = Date()
        var results: [Suggestion] = []
        
        // Early stopping if taking too long
        for pred in baseModel.streamPredictions(input) {
            if Date().timeIntervalSince(start) > timeout {
                break
            }
            
            results.append(pred)
            
            if results.count >= maxSuggestions {
                break
            }
        }
        
        return results
    }
}

// LRU Cache for frequent queries
class LRUCache<Key: Hashable, Value> {
    private var cache: [Key: Node<Key, Value>] = [:]
    private var head: Node<Key, Value>?
    private var tail: Node<Key, Value>?
    private let capacity: Int
    
    init(capacity: Int) {
        self.capacity = capacity
    }
    
    func get(_ key: Key) -> Value? {
        guard let node = cache[key] else { return nil }
        moveToHead(node)
        return node.value
    }
    
    func set(_ key: Key, _ value: Value) {
        if let node = cache[key] {
            node.value = value
            moveToHead(node)
        } else {
            let newNode = Node(key: key, value: value)
            cache[key] = newNode
            addToHead(newNode)
            
            if cache.count > capacity {
                removeTail()
            }
        }
    }
    
    private class Node<K, V> {
        let key: K
        var value: V
        var prev: Node?
        var next: Node?
        
        init(key: K, value: V) {
            self.key = key
            self.value = value
        }
    }
    
    // ... moveToHead, addToHead, removeTail implementations
}
```

## Summary

1. **Compression**: Use 4-bit quantization + LOUDS + pruning ‚Üí 10-20MB
2. **Languages**: Separate optimized models for JP (with EN) and EN (pure)
3. **Dictionaries**: Boost system in Layer 2, learns from user
4. **Top-N**: Use min-heap + early stopping + caching ‚Üí <50ms

Your keyboard will be:
- ‚úÖ Small: <50MB total
- ‚úÖ Fast: <100ms predictions
- ‚úÖ Smart: Learns user preferences
- ‚úÖ Efficient: Top 30-50 suggestions only
